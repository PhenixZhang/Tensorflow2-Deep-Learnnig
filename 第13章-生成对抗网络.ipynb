{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59942394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "# from scipy.misc import toimage\n",
    "from imageio import imsave\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from dataset import make_anime_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "98c573f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        filter = 64\n",
    "        self.conv1 = layers.Conv2DTranspose(filter * 8, 4, 1, 'valid', use_bias = False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2DTranspose(filter * 4, 4, 2, 'same', use_bias = False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2DTranspose(filter * 2, 4, 2, 'same', use_bias = False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.conv4 = layers.Conv2DTranspose(filter * 1, 4, 2, 'same', use_bias = False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.conv5 = layers.Conv2DTranspose(3, 4, 2, 'same', use_bias = False)\n",
    "        \n",
    "    def call(self, inputs, training = None):\n",
    "        x = inputs\n",
    "        x = tf.reshape(x, (x.shape[0], 1, 1, x.shape[1]))\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.relu(self.bn1(self.conv1(x), training=training))\n",
    "        x = tf.nn.relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = tf.nn.relu(self.bn3(self.conv3(x), training=training))\n",
    "        x = tf.nn.relu(self.bn4(self.conv4(x), training=training))\n",
    "        x = self.conv5(x)\n",
    "        x = tf.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d2a2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        filter = 64\n",
    "        self.conv1 = layers.Conv2D(filter, 4, 2, 'valid', use_bias = False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(filter * 2, 4, 2, 'valid', use_bias = False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(filter * 4, 4, 2, 'valid', use_bias = False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.conv4 = layers.Conv2D(filter * 8, 3, 1, 'valid', use_bias = False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.conv5 = layers.Conv2D(filter * 16, 3, 1, 'valid', use_bias = False)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.pool = layers.GlobalAveragePooling2D()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(inputs),training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x),training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x),training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x),training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn5(self.conv5(x),training=training))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1216626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    loss = d_loss_fake + d_loss_real\n",
    "    return loss\n",
    "\n",
    "def celoss_ones(logits):\n",
    "    y = tf.ones_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def celoss_zeros(logits):\n",
    "    y = tf.zeros_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    loss = celoss_ones(d_fake_logits)\n",
    "    return loss\n",
    "\n",
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "#     toimage(final_image).save(image_path)\n",
    "    imsave(image_path, final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images num: 51223\n",
      "<PrefetchDataset shapes: (64, 64, 64, 3), types: tf.float32> (64, 64, 3)\n",
      "(64, 64, 64, 3) 1.0 -1.0\n",
      "0 d-loss: 1.4976342916488647 g-loss: 0.5409046411514282\n",
      "100 d-loss: 0.5926315784454346 g-loss: 1.4660078287124634\n",
      "200 d-loss: 1.2859300374984741 g-loss: 1.1056727170944214\n",
      "300 d-loss: 0.6222599744796753 g-loss: 1.830592155456543\n",
      "400 d-loss: 0.7236131429672241 g-loss: 1.8737250566482544\n",
      "500 d-loss: 1.1825542449951172 g-loss: 2.9941649436950684\n",
      "600 d-loss: 1.0746476650238037 g-loss: 1.9452667236328125\n",
      "700 d-loss: 1.1636590957641602 g-loss: 2.20737886428833\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(3333)\n",
    "np.random.seed(3333)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "assert tf.__version__.startswith('2.')\n",
    "\n",
    "z_dim = 100 # 隐藏向量z的长度\n",
    "epochs = 3000000 # 训练步数\n",
    "batch_size = 64 # batch size\n",
    "learning_rate = 0.0002\n",
    "is_training = True\n",
    "\n",
    "img_path = glob.glob(r'faces/*.jpg')\n",
    "print('images num:', len(img_path))\n",
    "# 构建数据集对象\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size, resize = 64)\n",
    "print(dataset, img_shape)\n",
    "sample = next(iter(dataset)) # 采样\n",
    "print(sample.shape, tf.reduce_max(sample).numpy(),\n",
    "      tf.reduce_min(sample).numpy())\n",
    "dataset = dataset.repeat(100) # 重复循环\n",
    "db_iter = iter(dataset)\n",
    "\n",
    "generator = Generator()\n",
    "generator.build(input_shape=(4, z_dim))\n",
    "discriminator = Discriminator()\n",
    "discriminator.build(input_shape=(4, 64, 64, 3))\n",
    "g_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "d_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "\n",
    "d_losses, g_losses = [],[]\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(1):\n",
    "        batch_z = tf.random.normal([batch_size, z_dim])\n",
    "        batch_x = next(db_iter)\n",
    "        with tf.GradientTape() as tape:\n",
    "            d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "    batch_z = tf.random.normal([batch_size, z_dim])\n",
    "    batch_x = next(db_iter)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss))\n",
    "        # 可视化\n",
    "        z = tf.random.normal([100, z_dim])\n",
    "        fake_image = generator(z, training=False)\n",
    "        img_path = os.path.join('gan_images', 'gan-%d.png'%epoch)\n",
    "        save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n",
    "\n",
    "        d_losses.append(float(d_loss))\n",
    "        g_losses.append(float(g_loss))\n",
    "\n",
    "        if epoch % 10000 == 1:\n",
    "            # print(d_losses)\n",
    "            # print(g_losses)\n",
    "            generator.save_weights('generator.ckpt')\n",
    "            discriminator.save_weights('discriminator.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "52ec6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.07164985]\n",
      " [-0.12412959]], shape=(2, 1), dtype=float32)\n",
      "(2, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers\n",
    "\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]\n",
    "        self.fc = layers.Dense(3*3*512)\n",
    "\n",
    "        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [z, 100] => [z, 3*3*512]\n",
    "        x = self.fc(inputs)\n",
    "        x = tf.reshape(x, [-1, 3, 3, 512])\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        #\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = self.conv3(x)\n",
    "        x = tf.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # [b, 64, 64, 3] => [b, 1]\n",
    "        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n",
    "\n",
    "        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        # [b, h, w ,c] => [b, -1]\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        x = tf.nn.leaky_relu(self.conv1(inputs))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n",
    "\n",
    "        # [b, h, w, c] => [b, -1]\n",
    "        x = self.flatten(x)\n",
    "        # [b, -1] => [b, 1]\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def main():\n",
    "\n",
    "    d = Discriminator()\n",
    "    g = Generator()\n",
    "    x = tf.random.normal([2, 64, 64, 3])\n",
    "    z = tf.random.normal([2, 100])\n",
    "    prob = d(x)\n",
    "    print(prob)\n",
    "    x_hat = g(z)\n",
    "    print(x_hat.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, batch_x, fake_image):\n",
    "\n",
    "    batchsz = batch_x.shape[0]\n",
    "\n",
    "    # [b, h, w, c]\n",
    "    t = tf.random.uniform([batchsz, 1, 1, 1])\n",
    "    # [b, 1, 1, 1] => [b, h, w, c]\n",
    "    t = tf.broadcast_to(t, batch_x.shape)\n",
    "\n",
    "    interplate = t * batch_x + (1 - t) * fake_image\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([interplate])\n",
    "        d_interplote_logits = discriminator(interplate, training=True)\n",
    "    grads = tape.gradient(d_interplote_logits, interplate)\n",
    "\n",
    "    # grads:[b, h, w, c] => [b, -1]\n",
    "    grads = tf.reshape(grads, [grads.shape[0], -1])\n",
    "    gp = tf.norm(grads, axis=1) #[b]\n",
    "    gp = tf.reduce_mean( (gp-1)**2 )\n",
    "\n",
    "    return gp\n",
    "\n",
    "\n",
    "\n",
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    # 1. treat real image as real\n",
    "    # 2. treat generated image as fake\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    gp = gradient_penalty(discriminator, batch_x, fake_image)\n",
    "\n",
    "    loss = d_loss_real + d_loss_fake + 10. * gp\n",
    "\n",
    "    return loss, gp\n",
    "\n",
    "\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    loss = celoss_ones(d_fake_logits)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    tf.random.set_seed(233)\n",
    "    np.random.seed(233)\n",
    "    assert tf.__version__.startswith('2.')\n",
    "\n",
    "\n",
    "    # hyper parameters\n",
    "    z_dim = 100\n",
    "    epochs = 3000000\n",
    "    batch_size = 512\n",
    "    learning_rate = 0.0005\n",
    "    is_training = True\n",
    "\n",
    "\n",
    "    img_path = glob.glob(r'faces/*.jpg')\n",
    "    assert len(img_path) > 0\n",
    "    \n",
    "\n",
    "    dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)\n",
    "    print(dataset, img_shape)\n",
    "    sample = next(iter(dataset))\n",
    "    print(sample.shape, tf.reduce_max(sample).numpy(),\n",
    "          tf.reduce_min(sample).numpy())\n",
    "    dataset = dataset.repeat()\n",
    "    db_iter = iter(dataset)\n",
    "\n",
    "\n",
    "    generator = Generator() \n",
    "    generator.build(input_shape = (None, z_dim))\n",
    "    discriminator = Discriminator()\n",
    "    discriminator.build(input_shape=(None, 64, 64, 3))\n",
    "    z_sample = tf.random.normal([100, z_dim])\n",
    "\n",
    "\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for _ in range(5):\n",
    "            batch_z = tf.random.normal([batch_size, z_dim])\n",
    "            batch_x = next(db_iter)\n",
    "\n",
    "            # train D\n",
    "            with tf.GradientTape() as tape:\n",
    "                d_loss, gp = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "            grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "        \n",
    "        batch_z = tf.random.normal([batch_size, z_dim])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "        grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss),\n",
    "                  'gp:', float(gp))\n",
    "\n",
    "            z = tf.random.normal([100, z_dim])\n",
    "            fake_image = generator(z, training=False)\n",
    "            img_path = os.path.join('wgan-images', 'wgan-%d.png'%epoch)\n",
    "            save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa0fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
