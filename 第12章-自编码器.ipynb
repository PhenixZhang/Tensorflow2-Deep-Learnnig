{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c432cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Sequential, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1f9499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "h_dim = 20\n",
    "batchsz = 512\n",
    "lr = 1e-3\n",
    "\n",
    "def save_images(imgs, name):\n",
    "    new_im = Image.new('L', (280, 280))\n",
    "    index = 0\n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = imgs[index]\n",
    "            im = Image.fromarray(im, mode = 'L')\n",
    "            new_im.paste(im, (i, j))\n",
    "            index += 1\n",
    "    new_im.save(name)\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train ,x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_db = train_db.shuffle(batchsz * 5).batch(batchsz)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_db = test_db.batch(batchsz)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b160b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(256, activation = tf.nn.relu),\n",
    "            layers.Dense(128, activation = tf.nn.relu),\n",
    "            layers.Dense(h_dim)\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(128, activation = tf.nn.relu),\n",
    "            layers.Dense(256, activation = tf.nn.relu),\n",
    "            layers.Dense(784)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training = None):\n",
    "        h = self.encoder(inputs)\n",
    "        x_hat = self.decoder(h)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afcfc8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_6 (Sequential)    (4, 20)                   236436    \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (4, 784)                  237200    \n",
      "=================================================================\n",
      "Total params: 473,636\n",
      "Trainable params: 473,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 0 0.6941691637039185\n",
      "0 100 0.3297346830368042\n",
      "1 0 0.3226393461227417\n",
      "1 100 0.29958394169807434\n",
      "2 0 0.30743956565856934\n",
      "2 100 0.2934790253639221\n",
      "3 0 0.2952147126197815\n",
      "3 100 0.2991750240325928\n",
      "4 0 0.2829587459564209\n",
      "4 100 0.29336726665496826\n",
      "5 0 0.29115134477615356\n",
      "5 100 0.29069268703460693\n",
      "6 0 0.28854429721832275\n",
      "6 100 0.29182589054107666\n",
      "7 0 0.2849160134792328\n",
      "7 100 0.2867415249347687\n",
      "8 0 0.2843138873577118\n",
      "8 100 0.2800070643424988\n",
      "9 0 0.28185465931892395\n",
      "9 100 0.2828569710254669\n",
      "10 0 0.2752522826194763\n",
      "10 100 0.27593308687210083\n",
      "11 0 0.2741019129753113\n",
      "11 100 0.27952393889427185\n",
      "12 0 0.28389179706573486\n",
      "12 100 0.2814488708972931\n",
      "13 0 0.27803540229797363\n",
      "13 100 0.2861007750034332\n",
      "14 0 0.272881418466568\n",
      "14 100 0.27415481209754944\n",
      "15 0 0.2714199423789978\n",
      "15 100 0.28146159648895264\n",
      "16 0 0.26942765712738037\n",
      "16 100 0.28604814410209656\n",
      "17 0 0.26494452357292175\n",
      "17 100 0.27958598732948303\n",
      "18 0 0.2769750654697418\n",
      "18 100 0.2727668285369873\n",
      "19 0 0.26784244179725647\n",
      "19 100 0.28278762102127075\n",
      "20 0 0.2775249183177948\n",
      "20 100 0.2697116732597351\n",
      "21 0 0.2797394096851349\n",
      "21 100 0.2798190116882324\n",
      "22 0 0.27475589513778687\n",
      "22 100 0.2806400656700134\n",
      "23 0 0.2711420953273773\n",
      "23 100 0.2728806436061859\n",
      "24 0 0.26941031217575073\n",
      "24 100 0.272020548582077\n",
      "25 0 0.27286601066589355\n",
      "25 100 0.2866879999637604\n",
      "26 0 0.27926909923553467\n",
      "26 100 0.2802378535270691\n",
      "27 0 0.2696945071220398\n",
      "27 100 0.26759961247444153\n",
      "28 0 0.2742852568626404\n",
      "28 100 0.2772441506385803\n",
      "29 0 0.2687397301197052\n",
      "29 100 0.27565285563468933\n",
      "30 0 0.2721790671348572\n",
      "30 100 0.2744472920894623\n",
      "31 0 0.2667350769042969\n",
      "31 100 0.27637651562690735\n",
      "32 0 0.26781123876571655\n",
      "32 100 0.2672024071216583\n",
      "33 0 0.2746361196041107\n",
      "33 100 0.27849632501602173\n",
      "34 0 0.27346065640449524\n",
      "34 100 0.27278584241867065\n",
      "35 0 0.276833176612854\n",
      "35 100 0.2800997793674469\n",
      "36 0 0.26813361048698425\n",
      "36 100 0.28134316205978394\n",
      "37 0 0.27302849292755127\n",
      "37 100 0.273563414812088\n",
      "38 0 0.2634437084197998\n",
      "38 100 0.28110215067863464\n",
      "39 0 0.26826533675193787\n",
      "39 100 0.2741481363773346\n",
      "40 0 0.27854782342910767\n",
      "40 100 0.2683790922164917\n",
      "41 0 0.2724549174308777\n",
      "41 100 0.2731640040874481\n",
      "42 0 0.26373109221458435\n",
      "42 100 0.2745940089225769\n",
      "43 0 0.26934510469436646\n",
      "43 100 0.27126672863960266\n",
      "44 0 0.2703803777694702\n",
      "44 100 0.27432355284690857\n",
      "45 0 0.27232760190963745\n",
      "45 100 0.2737267017364502\n",
      "46 0 0.2621380686759949\n",
      "46 100 0.2729434370994568\n",
      "47 0 0.26482507586479187\n",
      "47 100 0.27074024081230164\n",
      "48 0 0.2719336450099945\n",
      "48 100 0.26783329248428345\n",
      "49 0 0.26514896750450134\n",
      "49 100 0.27327394485473633\n",
      "50 0 0.2695680856704712\n",
      "50 100 0.27299314737319946\n",
      "51 0 0.2697799503803253\n",
      "51 100 0.2715224623680115\n",
      "52 0 0.2732728123664856\n",
      "52 100 0.27166369557380676\n",
      "53 0 0.2651396691799164\n",
      "53 100 0.2725970149040222\n",
      "54 0 0.2637072205543518\n",
      "54 100 0.2660801410675049\n",
      "55 0 0.27372997999191284\n",
      "55 100 0.2714408040046692\n",
      "56 0 0.2657715976238251\n",
      "56 100 0.26925912499427795\n",
      "57 0 0.2674359083175659\n",
      "57 100 0.27247878909111023\n",
      "58 0 0.26618435978889465\n",
      "58 100 0.2710933983325958\n",
      "59 0 0.2683129906654358\n",
      "59 100 0.269175261259079\n",
      "60 0 0.2727433145046234\n",
      "60 100 0.2648419737815857\n",
      "61 0 0.27331939339637756\n",
      "61 100 0.2698325216770172\n",
      "62 0 0.26093265414237976\n",
      "62 100 0.26827114820480347\n",
      "63 0 0.2658752202987671\n",
      "63 100 0.2692544162273407\n",
      "64 0 0.2672380805015564\n",
      "64 100 0.2732260227203369\n",
      "65 0 0.2679547965526581\n",
      "65 100 0.2721938490867615\n",
      "66 0 0.2619643211364746\n",
      "66 100 0.2706906795501709\n",
      "67 0 0.2684464156627655\n",
      "67 100 0.2722814083099365\n",
      "68 0 0.2656625509262085\n",
      "68 100 0.269863486289978\n",
      "69 0 0.2652360796928406\n",
      "69 100 0.27053892612457275\n",
      "70 0 0.2670341730117798\n",
      "70 100 0.26779019832611084\n",
      "71 0 0.26579487323760986\n",
      "71 100 0.26713836193084717\n",
      "72 0 0.26356351375579834\n",
      "72 100 0.27259525656700134\n",
      "73 0 0.2674321234226227\n",
      "73 100 0.2775319516658783\n",
      "74 0 0.266262024641037\n",
      "74 100 0.26711586117744446\n",
      "75 0 0.26219168305397034\n",
      "75 100 0.27872908115386963\n",
      "76 0 0.26909688115119934\n",
      "76 100 0.26551127433776855\n",
      "77 0 0.27032583951950073\n",
      "77 100 0.2722923755645752\n",
      "78 0 0.26910272240638733\n",
      "78 100 0.26816534996032715\n",
      "79 0 0.2628990113735199\n",
      "79 100 0.2673354148864746\n",
      "80 0 0.2747403681278229\n",
      "80 100 0.2716407775878906\n",
      "81 0 0.267758309841156\n",
      "81 100 0.2676033675670624\n",
      "82 0 0.2705017924308777\n",
      "82 100 0.27366775274276733\n",
      "83 0 0.2673797905445099\n",
      "83 100 0.2737795114517212\n",
      "84 0 0.27415990829467773\n",
      "84 100 0.2703908383846283\n",
      "85 0 0.2668662965297699\n",
      "85 100 0.2678529918193817\n",
      "86 0 0.26828908920288086\n",
      "86 100 0.2778559625148773\n",
      "87 0 0.26729047298431396\n",
      "87 100 0.27069905400276184\n",
      "88 0 0.2639335095882416\n",
      "88 100 0.27517932653427124\n",
      "89 0 0.2730540335178375\n",
      "89 100 0.2738717496395111\n",
      "90 0 0.2660422921180725\n",
      "90 100 0.2696610987186432\n",
      "91 0 0.2660658061504364\n",
      "91 100 0.27520284056663513\n",
      "92 0 0.26868030428886414\n",
      "92 100 0.2743019163608551\n",
      "93 0 0.2644129693508148\n",
      "93 100 0.27281495928764343\n",
      "94 0 0.264281302690506\n",
      "94 100 0.27094778418540955\n",
      "95 0 0.26060089468955994\n",
      "95 100 0.26539692282676697\n",
      "96 0 0.25985732674598694\n",
      "96 100 0.2690768837928772\n",
      "97 0 0.2666376531124115\n",
      "97 100 0.26746866106987\n",
      "98 0 0.2622393071651459\n",
      "98 100 0.26561689376831055\n",
      "99 0 0.266033411026001\n",
      "99 100 0.2647382616996765\n"
     ]
    }
   ],
   "source": [
    "model = AE()\n",
    "model.build(input_shape=(4, 784))\n",
    "model.summary()\n",
    "optimizer = optimizers.Adam(lr = lr)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step, x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_rec_logits = model(x)\n",
    "            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n",
    "            rec_loss = tf.reduce_mean(rec_loss)\n",
    "        grads = tape.gradient(rec_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, float(rec_loss))\n",
    "            \n",
    "        x = next(iter(test_db))\n",
    "        logits = model(tf.reshape(x, [-1, 784]))\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28])\n",
    "        x_concat = tf.concat([x[:50],x_hat[:50]], axis=0)\n",
    "        x_concat = x_concat.numpy() * 255.\n",
    "        x_concat = x_concat.astype(np.uint8)\n",
    "        save_images(x_concat, 'ae_images/rec_epoch_%d.png'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c93c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __ini__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        #  Encoder 网络\n",
    "        self.fc1 = layers.Dense(128)\n",
    "        self.fc2 = layers.Dense(z_dim)\n",
    "        self.fc3 = layers.Dense(z_dim)\n",
    "        # Decoder 网络\n",
    "        self.fc4 = layers.Dense(128)\n",
    "        self.fc5 = layers.Dense(784)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        mu = self.fc2(h)\n",
    "        log_var = self.fc3(h)\n",
    "        \n",
    "        return mu, log_var\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        out = tf.nn.relu(self.fc4(z))\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        std = tf.exp(log_var) ** 0.5\n",
    "        z = mu + std + eps\n",
    "        return z\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        mu, log_var = self.encoder(inputs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c893b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 kl div: 2.400975227355957 rec_loss: 545.963134765625\n",
      "0 100 kl div: 15.316713333129883 rec_loss: 291.8880615234375\n",
      "1 0 kl div: 15.32813835144043 rec_loss: 283.68939208984375\n",
      "1 100 kl div: 15.231584548950195 rec_loss: 262.5343933105469\n",
      "2 0 kl div: 15.452970504760742 rec_loss: 255.45103454589844\n",
      "2 100 kl div: 14.357110977172852 rec_loss: 247.87405395507812\n",
      "3 0 kl div: 14.484508514404297 rec_loss: 247.49111938476562\n",
      "3 100 kl div: 14.951042175292969 rec_loss: 251.74859619140625\n",
      "4 0 kl div: 14.298286437988281 rec_loss: 245.4153594970703\n",
      "4 100 kl div: 14.243423461914062 rec_loss: 241.90908813476562\n",
      "5 0 kl div: 14.655537605285645 rec_loss: 242.5178680419922\n",
      "5 100 kl div: 14.697585105895996 rec_loss: 237.09083557128906\n",
      "6 0 kl div: 14.514230728149414 rec_loss: 235.9995574951172\n",
      "6 100 kl div: 14.402326583862305 rec_loss: 236.97702026367188\n",
      "7 0 kl div: 14.578542709350586 rec_loss: 238.581787109375\n",
      "7 100 kl div: 14.738691329956055 rec_loss: 241.0812530517578\n",
      "8 0 kl div: 14.994415283203125 rec_loss: 241.6929473876953\n",
      "8 100 kl div: 14.741355895996094 rec_loss: 233.17955017089844\n",
      "9 0 kl div: 14.637262344360352 rec_loss: 236.59194946289062\n",
      "9 100 kl div: 14.825458526611328 rec_loss: 239.1204376220703\n",
      "10 0 kl div: 14.463815689086914 rec_loss: 232.3942413330078\n",
      "10 100 kl div: 14.791790008544922 rec_loss: 240.49508666992188\n",
      "11 0 kl div: 14.446832656860352 rec_loss: 235.8768310546875\n",
      "11 100 kl div: 14.998697280883789 rec_loss: 233.4998016357422\n",
      "12 0 kl div: 14.801328659057617 rec_loss: 232.320068359375\n",
      "12 100 kl div: 15.232751846313477 rec_loss: 236.6277313232422\n",
      "13 0 kl div: 14.792232513427734 rec_loss: 232.65206909179688\n",
      "13 100 kl div: 14.604089736938477 rec_loss: 236.96324157714844\n",
      "14 0 kl div: 14.626556396484375 rec_loss: 233.53140258789062\n",
      "14 100 kl div: 14.729971885681152 rec_loss: 235.69558715820312\n",
      "15 0 kl div: 14.35322380065918 rec_loss: 229.22767639160156\n",
      "15 100 kl div: 14.509634017944336 rec_loss: 235.3997039794922\n",
      "16 0 kl div: 15.487648963928223 rec_loss: 229.34710693359375\n",
      "16 100 kl div: 14.831006050109863 rec_loss: 231.455810546875\n",
      "17 0 kl div: 14.67270278930664 rec_loss: 231.67489624023438\n",
      "17 100 kl div: 14.96105670928955 rec_loss: 234.31396484375\n",
      "18 0 kl div: 14.893665313720703 rec_loss: 233.70944213867188\n",
      "18 100 kl div: 14.521397590637207 rec_loss: 233.02951049804688\n",
      "19 0 kl div: 14.684630393981934 rec_loss: 231.6674346923828\n",
      "19 100 kl div: 14.716135025024414 rec_loss: 229.65399169921875\n",
      "20 0 kl div: 15.226366996765137 rec_loss: 224.8199462890625\n",
      "20 100 kl div: 14.800591468811035 rec_loss: 228.64370727539062\n",
      "21 0 kl div: 14.852570533752441 rec_loss: 226.4412384033203\n",
      "21 100 kl div: 15.296186447143555 rec_loss: 228.14474487304688\n",
      "22 0 kl div: 14.684052467346191 rec_loss: 228.4898223876953\n",
      "22 100 kl div: 15.153615951538086 rec_loss: 228.12083435058594\n",
      "23 0 kl div: 14.694339752197266 rec_loss: 224.46621704101562\n",
      "23 100 kl div: 14.857928276062012 rec_loss: 227.8255615234375\n",
      "24 0 kl div: 14.879623413085938 rec_loss: 228.6970977783203\n",
      "24 100 kl div: 14.804641723632812 rec_loss: 233.77195739746094\n",
      "25 0 kl div: 15.007858276367188 rec_loss: 230.52239990234375\n",
      "25 100 kl div: 15.19062614440918 rec_loss: 228.8624267578125\n",
      "26 0 kl div: 14.4160795211792 rec_loss: 232.90756225585938\n",
      "26 100 kl div: 14.774738311767578 rec_loss: 232.0323944091797\n",
      "27 0 kl div: 15.793539047241211 rec_loss: 227.97772216796875\n",
      "27 100 kl div: 15.100030899047852 rec_loss: 231.28607177734375\n",
      "28 0 kl div: 14.715723037719727 rec_loss: 224.785888671875\n",
      "28 100 kl div: 15.140223503112793 rec_loss: 230.62124633789062\n",
      "29 0 kl div: 15.044774055480957 rec_loss: 229.33717346191406\n",
      "29 100 kl div: 15.175745964050293 rec_loss: 234.87550354003906\n",
      "30 0 kl div: 15.035847663879395 rec_loss: 229.35293579101562\n",
      "30 100 kl div: 15.106182098388672 rec_loss: 229.1146697998047\n",
      "31 0 kl div: 15.250391960144043 rec_loss: 229.79830932617188\n",
      "31 100 kl div: 15.28540325164795 rec_loss: 223.06497192382812\n",
      "32 0 kl div: 15.24860954284668 rec_loss: 224.5467987060547\n",
      "32 100 kl div: 15.219804763793945 rec_loss: 225.29519653320312\n",
      "33 0 kl div: 15.034215927124023 rec_loss: 226.29415893554688\n",
      "33 100 kl div: 15.186140060424805 rec_loss: 232.9080810546875\n",
      "34 0 kl div: 14.98253059387207 rec_loss: 224.20281982421875\n",
      "34 100 kl div: 15.391008377075195 rec_loss: 234.3777618408203\n",
      "35 0 kl div: 14.891905784606934 rec_loss: 226.808837890625\n",
      "35 100 kl div: 15.0672607421875 rec_loss: 227.18283081054688\n",
      "36 0 kl div: 14.699975967407227 rec_loss: 233.01193237304688\n",
      "36 100 kl div: 15.303019523620605 rec_loss: 232.55252075195312\n",
      "37 0 kl div: 15.302718162536621 rec_loss: 227.20135498046875\n",
      "37 100 kl div: 14.850088119506836 rec_loss: 225.4103546142578\n",
      "38 0 kl div: 15.011255264282227 rec_loss: 227.5157470703125\n",
      "38 100 kl div: 15.143749237060547 rec_loss: 232.1671600341797\n",
      "39 0 kl div: 14.449444770812988 rec_loss: 221.2225341796875\n",
      "39 100 kl div: 15.10334587097168 rec_loss: 233.16949462890625\n",
      "40 0 kl div: 14.71296501159668 rec_loss: 228.52825927734375\n",
      "40 100 kl div: 15.076147079467773 rec_loss: 226.1082763671875\n",
      "41 0 kl div: 15.257441520690918 rec_loss: 228.42572021484375\n",
      "41 100 kl div: 14.821722030639648 rec_loss: 233.73545837402344\n",
      "42 0 kl div: 15.235613822937012 rec_loss: 225.01950073242188\n",
      "42 100 kl div: 15.366504669189453 rec_loss: 228.53675842285156\n",
      "43 0 kl div: 15.13582992553711 rec_loss: 225.58474731445312\n",
      "43 100 kl div: 15.625450134277344 rec_loss: 229.2913818359375\n",
      "44 0 kl div: 14.3614501953125 rec_loss: 227.9139404296875\n",
      "44 100 kl div: 15.046734809875488 rec_loss: 227.5634765625\n",
      "45 0 kl div: 14.829042434692383 rec_loss: 221.69134521484375\n",
      "45 100 kl div: 15.068299293518066 rec_loss: 229.7836456298828\n",
      "46 0 kl div: 15.378116607666016 rec_loss: 224.25558471679688\n",
      "46 100 kl div: 15.53187370300293 rec_loss: 230.60165405273438\n",
      "47 0 kl div: 15.075332641601562 rec_loss: 229.50778198242188\n",
      "47 100 kl div: 15.229275703430176 rec_loss: 232.1805419921875\n",
      "48 0 kl div: 14.63906478881836 rec_loss: 221.2006072998047\n",
      "48 100 kl div: 14.772750854492188 rec_loss: 231.9480743408203\n",
      "49 0 kl div: 15.093682289123535 rec_loss: 227.90769958496094\n",
      "49 100 kl div: 15.119542121887207 rec_loss: 228.78443908691406\n",
      "50 0 kl div: 15.643885612487793 rec_loss: 226.7256622314453\n",
      "50 100 kl div: 15.205187797546387 rec_loss: 227.33441162109375\n",
      "51 0 kl div: 15.086747169494629 rec_loss: 225.28123474121094\n",
      "51 100 kl div: 15.465057373046875 rec_loss: 225.1796417236328\n",
      "52 0 kl div: 15.289434432983398 rec_loss: 226.02114868164062\n",
      "52 100 kl div: 15.190292358398438 rec_loss: 225.54335021972656\n",
      "53 0 kl div: 15.442252159118652 rec_loss: 231.6136932373047\n",
      "53 100 kl div: 15.464163780212402 rec_loss: 232.685791015625\n",
      "54 0 kl div: 15.161836624145508 rec_loss: 226.57177734375\n",
      "54 100 kl div: 15.530620574951172 rec_loss: 226.5598907470703\n",
      "55 0 kl div: 15.126960754394531 rec_loss: 222.39923095703125\n",
      "55 100 kl div: 14.694205284118652 rec_loss: 229.46176147460938\n",
      "56 0 kl div: 15.048294067382812 rec_loss: 224.62364196777344\n",
      "56 100 kl div: 15.532051086425781 rec_loss: 232.7579345703125\n",
      "57 0 kl div: 14.712757110595703 rec_loss: 226.98760986328125\n",
      "57 100 kl div: 15.061776161193848 rec_loss: 232.88671875\n",
      "58 0 kl div: 15.153382301330566 rec_loss: 224.97158813476562\n",
      "58 100 kl div: 15.139097213745117 rec_loss: 228.29061889648438\n",
      "59 0 kl div: 15.164373397827148 rec_loss: 233.50656127929688\n",
      "59 100 kl div: 15.396781921386719 rec_loss: 229.8272705078125\n",
      "60 0 kl div: 14.961113929748535 rec_loss: 224.145263671875\n",
      "60 100 kl div: 14.71818733215332 rec_loss: 232.2787628173828\n",
      "61 0 kl div: 14.550359725952148 rec_loss: 228.39974975585938\n",
      "61 100 kl div: 15.361286163330078 rec_loss: 228.99313354492188\n",
      "62 0 kl div: 15.120550155639648 rec_loss: 223.86033630371094\n",
      "62 100 kl div: 14.823102951049805 rec_loss: 222.20323181152344\n",
      "63 0 kl div: 14.812238693237305 rec_loss: 225.07606506347656\n",
      "63 100 kl div: 15.206005096435547 rec_loss: 229.37918090820312\n",
      "64 0 kl div: 15.154337882995605 rec_loss: 224.01779174804688\n",
      "64 100 kl div: 15.023765563964844 rec_loss: 229.80641174316406\n",
      "65 0 kl div: 15.449342727661133 rec_loss: 222.9384002685547\n",
      "65 100 kl div: 15.448582649230957 rec_loss: 234.3151397705078\n",
      "66 0 kl div: 14.669717788696289 rec_loss: 227.90496826171875\n",
      "66 100 kl div: 14.93543529510498 rec_loss: 231.196044921875\n",
      "67 0 kl div: 15.285293579101562 rec_loss: 223.61036682128906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 100 kl div: 15.518753051757812 rec_loss: 227.69898986816406\n",
      "68 0 kl div: 14.833165168762207 rec_loss: 224.1849365234375\n",
      "68 100 kl div: 14.959325790405273 rec_loss: 230.2875213623047\n",
      "69 0 kl div: 15.196447372436523 rec_loss: 228.95762634277344\n",
      "69 100 kl div: 15.017204284667969 rec_loss: 227.50967407226562\n",
      "70 0 kl div: 14.849920272827148 rec_loss: 220.37762451171875\n",
      "70 100 kl div: 15.533491134643555 rec_loss: 225.40155029296875\n",
      "71 0 kl div: 14.781791687011719 rec_loss: 226.70831298828125\n",
      "71 100 kl div: 14.774552345275879 rec_loss: 228.6650848388672\n",
      "72 0 kl div: 14.753569602966309 rec_loss: 226.87274169921875\n",
      "72 100 kl div: 14.628839492797852 rec_loss: 228.13720703125\n",
      "73 0 kl div: 15.018328666687012 rec_loss: 223.79971313476562\n",
      "73 100 kl div: 15.17948055267334 rec_loss: 229.6072998046875\n",
      "74 0 kl div: 15.067667007446289 rec_loss: 223.3785858154297\n",
      "74 100 kl div: 15.305872917175293 rec_loss: 223.47813415527344\n",
      "75 0 kl div: 14.593851089477539 rec_loss: 223.14056396484375\n",
      "75 100 kl div: 14.877875328063965 rec_loss: 225.414306640625\n",
      "76 0 kl div: 14.866817474365234 rec_loss: 218.80892944335938\n",
      "76 100 kl div: 14.613649368286133 rec_loss: 221.1321258544922\n",
      "77 0 kl div: 15.389815330505371 rec_loss: 225.0306396484375\n"
     ]
    }
   ],
   "source": [
    "z_dim = 10\n",
    "model = VAE()\n",
    "model.build(input_shape = (4, 784))\n",
    "optimizer = optimizers.Adam(lr)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step, x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1,784])\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_rec_logits, mu, log_var = model(x)\n",
    "            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x,logits=x_rec_logits)\n",
    "            rec_loss = tf.reduce_sum(rec_loss) / x.shape[0]\n",
    "            kl_div = -0.5 * (log_var + 1 - mu ** 2 - tf.exp(log_var))\n",
    "            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n",
    "            loss = rec_loss + 1. * kl_div\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, 'kl div:', float(kl_div), 'rec_loss:', float(rec_loss))\n",
    "                \n",
    "    # evaluation\n",
    "    z = tf.random.normal((batchsz, z_dim))\n",
    "    logits = model.decoder(z)\n",
    "    x_hat = tf.sigmoid(logits)\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n",
    "    x_hat = x_hat.astype(np.uint8)\n",
    "    save_images(x_hat, 'vae_images/sampled_epoch%d.png'%epoch)\n",
    "\n",
    "    x = next(iter(test_db))\n",
    "    x = tf.reshape(x, [-1, 784])\n",
    "    x_hat_logits, _, _ = model(x)\n",
    "    x_hat = tf.sigmoid(x_hat_logits)\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n",
    "    x_hat = x_hat.astype(np.uint8)\n",
    "    save_images(x_hat, 'vae_images/rec_epoch%d.png'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23f8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
